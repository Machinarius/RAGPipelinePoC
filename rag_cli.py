import argparse
import os
import re
import sys
import html
from dotenv import load_dotenv

from chroma_client import get_chroma_client

# Ensure the required libraries are available
try:
    from docling.document_converter import DocumentConverter
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.text_splitter import MarkdownHeaderTextSplitter
    from langchain_chroma import Chroma
    from langchain_ollama import OllamaEmbeddings
    from langchain_core.documents import Document

    # OpenAI imports (will be None if not installed)
    try:
        from langchain_openai import OpenAIEmbeddings

        openai_available = True
    except ImportError:
        OpenAIEmbeddings = None
        openai_available = False

except ImportError:
    print(
        "Error: Required packages not installed. Run 'pip install -r requirements.txt'"
    )
    exit(1)

# --- Configuration ---
load_dotenv()
COLLECTION_NAME = os.getenv("COLLECTION_NAME")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")
CHROMA_HOST = os.getenv("CHROMA_HOST")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 100))
COLLECTION_NAME_PARENTS = os.getenv("COLLECTION_NAME_PARENTS")

# OpenAI specific configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")

chroma_client = get_chroma_client()

# Define the hierarchical structure for legal documents (Article -> Parágrafos)
HEADERS_TO_SPLIT_ON = [
    ("##", "ARTÍCULO"),
    ("###", "PARÁGRAFO"),
]


def create_embedding_model(use_openai: bool = False):
    """Factory function to create the appropriate embedding model"""
    if use_openai:
        if not openai_available:
            raise RuntimeError(
                "OpenAI flag specified but langchain-openai is not installed. "
                "Install with: pip install langchain-openai"
            )
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY is required for OpenAI embeddings")

        print("Using OpenAI embeddings...")
        return OpenAIEmbeddings(
            model=OPENAI_EMBEDDING_MODEL,
            api_key=OPENAI_API_KEY,
        )
    else:
        if not EMBEDDING_MODEL or not OLLAMA_BASE_URL:
            raise RuntimeError("Ollama configuration variables are required")

        print("Using Ollama embeddings...")
        return OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)


# --- Core RAG Functions ---


def clean_and_prefix_text(text: str) -> str:
    """
    Cleans up common formatting issues and prefixes headers with Markdown syntax
    to enable structured splitting.
    """
    # 0. Unescape HTML entities that may be present from document conversion
    text = html.unescape(text)

    # Remove any text within angle brackets (e.g., <Ver Notas de Vigencia>)
    text = re.sub(r"<.*?>", "", text)

    # 1. Standardize line breaks and remove common web/PDF footers
    text = text.replace("\n\n", "\n").replace("\r\n", "\n")

    # Remove URL/date footers often generated by PDF readers
    text = re.sub(r"file://.*?\n", "", text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r"\d{1,2}/\d{1,2}/\d{4}", "", text)

    # 2. Add Markdown Prefixes for structured splitting (used by MarkdownHeaderTextSplitter)
    text = re.sub(r"(\n|^)(ARTÍCULO\s+\d+)", r"\n## \2", text, flags=re.IGNORECASE)
    text = re.sub(
        r"(\n|^)(PARÁGRAFO\s+\d+o?\.)", r"\n### \2", text, flags=re.IGNORECASE
    )

    return text.strip()


def extract_page_info_from_docling_document(docling_doc) -> list[dict]:
    """
    Extract page information and content from Docling document.
    Returns a list of dictionaries with page content and metadata.
    """
    page_docs = []

    # Get the full text as markdown
    full_markdown = docling_doc.export_to_markdown()

    # Try to extract page information from the DoclingDocument structure
    # Docling provides page-level information through its document structure
    try:
        # Check if the document has page information
        if hasattr(docling_doc, "pages") and docling_doc.pages:
            # Process each page separately
            for page_idx, page in enumerate(docling_doc.pages):
                page_content = ""
                page_num = page_idx + 1

                # Extract text from page items
                for item in page.items:
                    if hasattr(item, "text") and item.text:
                        page_content += item.text + "\n"

                if page_content.strip():
                    # Clean and prefix the page content
                    cleaned_content = clean_and_prefix_text(page_content)

                    # Add page tag at the beginning
                    page_tag = f"[PAGINA_ORIGEN: {page_num}]\n\n"
                    final_content = page_tag + cleaned_content

                    page_docs.append(
                        {
                            "page_content": final_content,
                            "metadata": {
                                "page": page_num,
                                "source": "docling_processed",
                            },
                        }
                    )
        else:
            # Fallback: use the full markdown as a single document
            # Try to split by page indicators if they exist in the markdown
            page_pattern = r"(?=Page\s+\d+|PAGE\s+\d+|\f)"
            pages = re.split(page_pattern, full_markdown, flags=re.IGNORECASE)

            for page_idx, page_content in enumerate(pages):
                if page_content.strip():
                    page_num = page_idx + 1
                    cleaned_content = clean_and_prefix_text(page_content)

                    # Add page tag at the beginning
                    page_tag = f"[PAGINA_ORIGEN: {page_num}]\n\n"
                    final_content = page_tag + cleaned_content

                    page_docs.append(
                        {
                            "page_content": final_content,
                            "metadata": {
                                "page": page_num,
                                "source": "docling_processed",
                            },
                        }
                    )

            # If no pages were detected, treat the whole document as one page
            if not page_docs:
                cleaned_content = clean_and_prefix_text(full_markdown)
                page_tag = "[PAGINA_ORIGEN: 1]\n\n"
                final_content = page_tag + cleaned_content

                page_docs.append(
                    {
                        "page_content": final_content,
                        "metadata": {"page": 1, "source": "docling_processed"},
                    }
                )

    except Exception as e:
        print(f"Warning: Could not extract page information from DoclingDocument: {e}")
        # Fallback to treating whole document as single page
        cleaned_content = clean_and_prefix_text(full_markdown)
        page_tag = "[PAGINA_ORIGEN: 1]\n\n"
        final_content = page_tag + cleaned_content

        page_docs.append(
            {
                "page_content": final_content,
                "metadata": {"page": 1, "source": "docling_processed"},
            }
        )

    return page_docs


def ingest_pdf(file_path: str, use_openai: bool = False):
    """
    1. Loads the PDF using Docling's DocumentConverter for advanced parsing.
    2. Extracts page-level content and metadata.
    3. Applies hierarchical splitting (Parent-Child) logic and links them.
    4. Cleans metadata and extracts the page number from the content tag.
    5. Stores resulting documents in two ChromaDB collections.
    """
    provider = "OpenAI" if use_openai else "Ollama"
    print(
        f"--- Starting Docling-based Ingestion for: {file_path} (using {provider} embeddings) ---"
    )

    # 1. Document Loading with Docling
    print("Loading document with Docling...")
    converter = DocumentConverter()

    try:
        result = converter.convert(file_path)
        docling_doc = result.document
        print(f"Document loaded successfully. Status: {result.status}")
    except Exception as e:
        print(f"Error loading document with Docling: {e}")
        return

    # 2. Extract page-based documents from Docling document
    print("Extracting page information...")
    page_based_docs = extract_page_info_from_docling_document(docling_doc)
    print(f"Extracted {len(page_based_docs)} page(s) from document")

    # Convert to LangChain Document objects
    processed_docs = []
    for page_data in page_based_docs:
        doc = Document(
            page_content=page_data["page_content"], metadata=page_data["metadata"]
        )
        # Store original source for file name tracking
        doc.metadata["original_source"] = os.path.basename(file_path)
        processed_docs.append(doc)

    # 3. Hierarchical/Parent Splitting (Separating Articles and Parágrafos)
    print("Applying hierarchical splitting...")
    full_text = "\n\n".join([doc.page_content for doc in processed_docs])

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=HEADERS_TO_SPLIT_ON, strip_headers=False
    )
    parent_chunks = markdown_splitter.split_text(full_text)
    print(f"Created {len(parent_chunks)} parent chunks")

    # Dictionary to store parent chunks (Articles) for retrieval by ID
    parent_chunk_map: dict[str, Document] = {}

    # --- Prepare Parent Chunks ---
    for i, doc in enumerate(parent_chunks):
        # Create a unique ID for each Parent Chunk
        parent_id = f"parent_{i}_{hash(doc.page_content)}"
        doc.metadata["doc_id"] = parent_id
        parent_chunk_map[parent_id] = doc

    # 4. Child Splitting (Breaking large Parent Chunks into search-ready pieces)
    print("Creating child chunks...")
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )

    child_chunks: list[Document] = []
    for parent_doc in parent_chunks:
        # Split the parent text into child chunks
        splits = child_splitter.create_documents([parent_doc.page_content])

        for split in splits:
            # Propagate the Parent ID and Metadata to the Child Chunks
            split.metadata.update(parent_doc.metadata)
            # Add a unique ID for the child itself (optional, but good practice)
            split.metadata["parent_id"] = parent_doc.metadata["doc_id"]
            child_chunks.append(split)

    print(f"Created {len(child_chunks)} child chunks")

    # 5. Final Metadata Cleaning and Standardization
    print("Cleaning and standardizing metadata...")
    article_regex = re.compile(r"(ARTÍCULO\s+\d+)", re.IGNORECASE)
    paragrafo_regex = re.compile(r"(PARÁGRAFO\s+\d+o?\.)", re.IGNORECASE)
    page_tag_regex = re.compile(r"\[PAGINA_ORIGEN:\s*(\d+)\]", re.IGNORECASE)

    final_child_documents = []

    for doc in child_chunks:
        metadata = doc.metadata

        # --- A. Clean Citation Metadata (ARTÍCULO/PARÁGRAFO) ---
        if "ARTÍCULO" in metadata:
            match = article_regex.search(metadata["ARTÍCULO"])
            metadata["ARTÍCULO"] = match.group(1).upper() if match else None

        if "PARÁGRAFO" in metadata:
            match = paragrafo_regex.search(metadata["PARÁGRAFO"])
            metadata["PARÁGRAFO"] = match.group(1).upper() if match else None

        # --- B. EXTRACT Page Metadata from Content ---
        page_matches = page_tag_regex.findall(doc.page_content)

        if page_matches:
            pages = sorted(list(set(int(p) for p in page_matches)))
            metadata["page"] = pages[0]
            if len(pages) > 1:
                metadata["pages"] = ", ".join(map(str, pages))

            doc.page_content = page_tag_regex.sub("", doc.page_content).strip()
        else:
            metadata["page"] = None

        # --- C. Propagate Source Metadata ---
        if "original_source" in metadata:
            metadata["source"] = os.path.basename(metadata["original_source"])
        else:
            metadata["source"] = os.path.basename(file_path)

        keys_to_del = [
            "original_source",
            "original_page",
            "Header",
            "Unnamed: 0",
            "doc_id",
        ]  # doc_id is parent_id now
        for key in keys_to_del:
            if key in metadata:
                del metadata[key]

        # Ensure parent_id is stored in metadata for lookup in main.py
        if "parent_id" not in metadata:
            # This should not happen, but safety first
            metadata["parent_id"] = "unknown"

        metadata = {k: v for k, v in metadata.items() if v is not None}
        doc.metadata = metadata

        final_child_documents.append(doc)

    # 6. Store in Vector Database (Two Collections)
    print("Storing documents in vector database...")
    embed_model = create_embedding_model(use_openai)

    # --- Delete and Create Child Collection (Search Index) ---
    print(f"Deleting existing child collection '{COLLECTION_NAME}'...")
    try:
        Chroma(
            collection_name=COLLECTION_NAME,
            embedding_function=embed_model,
            client=chroma_client,
        ).delete_collection()
    except Exception as e:
        print(f"Warning: Could not delete child collection: {e}")

    print(f"Adding {len(final_child_documents)} child chunks to ChromaDB...")
    Chroma.from_documents(
        documents=final_child_documents,
        embedding=embed_model,
        collection_name=COLLECTION_NAME,
        client=chroma_client,
    )

    # --- Delete and Create Parent Collection (Context Index) ---
    print(f"Deleting existing parent collection '{COLLECTION_NAME_PARENTS}'...")
    try:
        Chroma(
            collection_name=COLLECTION_NAME_PARENTS,
            embedding_function=embed_model,
            client=chroma_client,
        ).delete_collection()
    except Exception as e:
        print(f"Warning: Could not delete parent collection: {e}")

    parent_documents_list = list(parent_chunk_map.values())
    parent_ids = [doc.metadata["doc_id"] for doc in parent_documents_list]

    print(f"Adding {len(parent_documents_list)} parent documents to ChromaDB...")

    Chroma(
        collection_name=COLLECTION_NAME_PARENTS,
        embedding_function=embed_model,
        client=chroma_client,
    ).add_documents(documents=parent_documents_list, ids=parent_ids)

    print(
        f"--- Docling Ingestion Complete. {len(final_child_documents)} children and {len(parent_documents_list)} parents indexed. ---"
    )


def run_server(port: int, use_openai: bool = False):
    """
    Runs the FastAPI RAG server.
    """
    provider = "OpenAI" if use_openai else "Ollama"
    print(f"--- 2. Starting FastAPI RAG Server with {provider} on port {port} ---")
    print("Navigate to http://localhost:8500/docs for the interactive API interface.")
    try:
        import uvicorn

        # Set environment variables expected by api_server.py, ensuring local server host is used
        os.environ["CHROMA_HOST"] = CHROMA_HOST
        os.environ["OLLAMA_BASE_URL"] = OLLAMA_BASE_URL

        # Build uvicorn command with OpenAI flag if needed
        import sys

        original_argv = sys.argv[:]
        try:
            # Modify sys.argv to include --openai flag for api_server.py argument parsing
            if use_openai:
                sys.argv = ["api_server.py", "--openai"]
            else:
                sys.argv = ["api_server.py"]

            uvicorn.run("api_server:app", host="127.0.0.1", port=port, reload=True)
        finally:
            # Restore original sys.argv
            sys.argv = original_argv

    except ImportError:
        print(
            "Error: Uvicorn not installed. This should not happen if requirements.txt was used."
        )
    except Exception as e:
        print(f"An error occurred while starting the server: {e}")
        print("Check if the port is already in use.")


def main():
    parser = argparse.ArgumentParser(
        description="Python RAG Prototype CLI with Docling."
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Ingest command
    ingest_parser = subparsers.add_parser(
        "ingest", help="Ingest a PDF file of regulations using Docling."
    )
    ingest_parser.add_argument(
        "pdf_path", type=str, help="Path to the PDF file (e.g., ./regulations.pdf)."
    )
    ingest_parser.add_argument(
        "--openai", action="store_true", help="Use OpenAI embeddings instead of Ollama"
    )

    # Server command
    server_parser = subparsers.add_parser("server", help="Run the FastAPI chat server.")
    server_parser.add_argument(
        "--port", type=int, default=8500, help="Port to run the FastAPI server on."
    )
    server_parser.add_argument(
        "--openai", action="store_true", help="Use OpenAI API instead of Ollama"
    )

    args = parser.parse_args()

    if args.command == "ingest":
        ingest_pdf(args.pdf_path, args.openai)
    elif args.command == "server":
        run_server(args.port, args.openai)


if __name__ == "__main__":
    main()
