import argparse
import os
import re
from dotenv import load_dotenv

from chroma_client import get_chroma_client

# Ensure the required libraries are available
try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.text_splitter import MarkdownHeaderTextSplitter
    from langchain_chroma import Chroma
    from langchain_ollama import OllamaEmbeddings
    from langchain_core.documents import Document
except ImportError:
    print(
        "Error: Required LangChain packages not installed. Run 'pip install -r requirements.txt'"
    )
    exit(1)

# --- Configuration ---
load_dotenv()
COLLECTION_NAME = os.getenv("COLLECTION_NAME")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")
CHROMA_HOST = os.getenv("CHROMA_HOST")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 100))
COLLECTION_NAME_PARENTS = os.getenv("COLLECTION_NAME_PARENTS")

chroma_client = get_chroma_client()

# Define the hierarchical structure for legal documents (Article -> Parágrafos)
HEADERS_TO_SPLIT_ON = [
    ("##", "ARTÍCULO"),
    ("###", "PARÁGRAFO"),
]

# --- Core RAG Functions ---


def clean_and_prefix_text(text: str) -> str:
    """
    Cleans up common formatting issues and prefixes headers with Markdown syntax
    to enable structured splitting.
    """
    # 1. Standardize line breaks and remove common web/PDF footers
    text = text.replace("\n\n", "\n").replace("\r\n", "\n")

    # Remove URL/date footers often generated by PDF readers
    text = re.sub(r"file://.*?\n", "", text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r"\d{1,2}/\d{1,2}/\d{4}", "", text)

    # 2. Add Markdown Prefixes for structured splitting (used by MarkdownHeaderTextSplitter)
    text = re.sub(r"(\n|^)(ARTÍCULO\s+\d+)", r"\n## \2", text, flags=re.IGNORECASE)
    text = re.sub(
        r"(\n|^)(PARÁGRAFO\s+\d+o?\.)", r"\n### \2", text, flags=re.IGNORECASE
    )

    return text.strip()


def ingest_pdf(file_path: str):
    """
    1. Loads the PDF and extracts initial metadata (page, source).
    2. Injects page metadata directly into the content for persistence (at the START of the page).
    3. Applies hierarchical splitting (Parent-Child) logic and links them.
    4. Cleans metadata and extracts the page number from the content tag.
    5. Stores resulting documents in two ChromaDB collections.
    """
    print(f"--- Starting Ingestion for: {file_path} ---")

    # 1. Initial Document Loading and Cleaning
    loader = PyPDFLoader(file_path)
    initial_docs = loader.load()

    processed_docs = []
    for doc in initial_docs:
        # Step 2: Clean text and inject page number into the content
        doc.page_content = clean_and_prefix_text(doc.page_content)

        # INJECT PAGE NUMBER DIRECTLY INTO CONTENT FOR PERSISTENCE (AT THE START)
        page_num = doc.metadata.get("page", 0) + 1
        page_tag = f"[PAGINA_ORIGEN: {page_num}]\n\n"

        # Prepend the page tag to the content
        doc.page_content = page_tag + doc.page_content

        # Store original source for file name tracking
        doc.metadata["original_source"] = doc.metadata.get(
            "source", os.path.basename(file_path)
        )

        processed_docs.append(doc)

    # 3. Hierarchical/Parent Splitting (Separating Articles and Parágrafos)

    full_text = "\n\n".join([doc.page_content for doc in processed_docs])

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=HEADERS_TO_SPLIT_ON, strip_headers=False
    )
    parent_chunks = markdown_splitter.split_text(full_text)

    # Dictionary to store parent chunks (Articles) for retrieval by ID
    parent_chunk_map: Dict[str, Document] = {}

    # --- Prepare Parent Chunks ---
    for i, doc in enumerate(parent_chunks):
        # Create a unique ID for each Parent Chunk
        parent_id = f"parent_{i}_{hash(doc.page_content)}"
        doc.metadata["doc_id"] = parent_id
        parent_chunk_map[parent_id] = doc

    # 4. Child Splitting (Breaking large Parent Chunks into search-ready pieces)
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )

    child_chunks: List[Document] = []
    for parent_doc in parent_chunks:
        # Split the parent text into child chunks
        splits = child_splitter.create_documents([parent_doc.page_content])

        for split in splits:
            # Propagate the Parent ID and Metadata to the Child Chunks
            split.metadata.update(parent_doc.metadata)
            # Add a unique ID for the child itself (optional, but good practice)
            split.metadata["parent_id"] = parent_doc.metadata["doc_id"]
            child_chunks.append(split)

    # 5. Final Metadata Cleaning and Standardization
    article_regex = re.compile(r"(ARTÍCULO\s+\d+)", re.IGNORECASE)
    paragrafo_regex = re.compile(r"(PARÁGRAFO\s+\d+o?\.)", re.IGNORECASE)
    page_tag_regex = re.compile(r"\[PAGINA_ORIGEN:\s*(\d+)\]", re.IGNORECASE)

    final_child_documents = []

    for doc in child_chunks:
        metadata = doc.metadata

        # --- A. Clean Citation Metadata (ARTÍCULO/PARÁGRAFO) ---
        # (Same cleaning logic as before)
        if "ARTÍCULO" in metadata:
            match = article_regex.search(metadata["ARTÍCULO"])
            metadata["ARTÍCULO"] = match.group(1).upper() if match else None

        if "PARÁGRAFO" in metadata:
            match = paragrafo_regex.search(metadata["PARÁGRAFO"])
            metadata["PARÁGRAFO"] = match.group(1).upper() if match else None

        # --- B. EXTRACT Page Metadata from Content ---
        page_match = page_tag_regex.search(doc.page_content)

        if page_match:
            metadata["page"] = int(page_match.group(1))
            doc.page_content = page_tag_regex.sub("", doc.page_content).strip()
        else:
            metadata["page"] = None

        # --- C. Propagate Source Metadata ---
        if "original_source" in metadata:
            metadata["source"] = os.path.basename(metadata["original_source"])
        else:
            metadata["source"] = os.path.basename(file_path)

        keys_to_del = [
            "original_source",
            "original_page",
            "Header",
            "Unnamed: 0",
            "doc_id",
        ]  # doc_id is parent_id now
        for key in keys_to_del:
            if key in metadata:
                del metadata[key]

        # Ensure parent_id is stored in metadata for lookup in main.py
        if "parent_id" not in metadata:
            # This should not happen, but safety first
            metadata["parent_id"] = "unknown"

        metadata = {k: v for k, v in metadata.items() if v is not None}
        doc.metadata = metadata

        final_child_documents.append(doc)

    # 6. Store in Vector Database (Two Collections)
    embed_model = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)

    # --- Delete and Create Child Collection (Search Index) ---
    print(f"Deleting existing child collection '{COLLECTION_NAME}'...")
    Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embed_model,
        client=chroma_client,
    ).delete_collection()

    print(f"Adding {len(final_child_documents)} child chunks to ChromaDB...")
    Chroma.from_documents(
        documents=final_child_documents,
        embedding=embed_model,
        collection_name=COLLECTION_NAME,
        client=chroma_client,
    )

    # --- Delete and Create Parent Collection (Context Index) ---
    # We store the parent chunks here, indexed by their custom ID (doc_id)
    print(f"Deleting existing parent collection '{COLLECTION_NAME_PARENTS}'...")
    Chroma(
        collection_name=COLLECTION_NAME_PARENTS,
        embedding_function=embed_model,
        client=chroma_client,
    ).delete_collection()

    parent_documents_list = list(parent_chunk_map.values())
    parent_ids = [doc.metadata["doc_id"] for doc in parent_documents_list]

    print(f"Adding {len(parent_documents_list)} parent documents to ChromaDB...")

    Chroma(
        collection_name=COLLECTION_NAME_PARENTS,
        embedding_function=embed_model,
        client=chroma_client,
    ).add_documents(documents=parent_documents_list, ids=parent_ids)

    print(
        f"--- Ingestion Complete. {len(final_child_documents)} children and {len(parent_documents_list)} parents indexed. ---"
    )


def run_server(port: int):
    """
    Runs the FastAPI RAG server.
    """
    print(f"--- 2. Starting FastAPI RAG Server on port {port} ---")
    print("Navigate to http://localhost:8500/docs for the interactive API interface.")
    try:
        import uvicorn

        # Set environment variables expected by main.py, ensuring local server host is used
        os.environ["CHROMA_HOST"] = CHROMA_HOST
        os.environ["OLLAMA_BASE_URL"] = OLLAMA_BASE_URL
        uvicorn.run("api_server:app", host="127.0.0.1", port=port, reload=True)
    except ImportError:
        print(
            "Error: Uvicorn not installed. This should not happen if requirements.txt was used."
        )
    except Exception as e:
        print(f"An error occurred while starting the server: {e}")
        print("Check if the port is already in use.")


def main():
    parser = argparse.ArgumentParser(description="Python RAG Prototype CLI.")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Ingest command
    ingest_parser = subparsers.add_parser(
        "ingest", help="Ingest a PDF file of regulations."
    )
    ingest_parser.add_argument(
        "pdf_path", type=str, help="Path to the PDF file (e.g., ./regulations.pdf)."
    )

    # Server command
    server_parser = subparsers.add_parser("server", help="Run the FastAPI chat server.")
    server_parser.add_argument(
        "--port", type=int, default=8500, help="Port to run the FastAPI server on."
    )

    args = parser.parse_args()

    if args.command == "ingest":
        ingest_pdf(args.pdf_path)
    elif args.command == "server":
        run_server(args.port)


if __name__ == "__main__":
    main()
