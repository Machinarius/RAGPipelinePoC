# --- AI Configuration ---
# The embedding model used for RAG (must match model pulled in docker-compose)

EMBEDDING_MODEL=embeddinggemma:300m
# The LLM used for final answer generation (must match model pulled in docker-compose)

GENERATION_MODEL=gemma2:2b
# --- Service Endpoints (Internal Docker Network Names) ---
# Note: When running outside of Docker (like the ingest script), use localhost.

CHROMA_HOST=http://localhost:8000
OLLAMA_BASE_URL=http://localhost:11434
# --- RAG Parameters ---
# The name of the vector collection/index where regulations are stored

COLLECTION_NAME=transit_regulations
COLLECTION_NAME_CHILDREN=transit_regulations
COLLECTION_NAME_PARENTS=transit_regulations_parents
# Size of text chunks (in characters) and overlap between them

CHUNK_SIZE=1000
CHUNK_OVERLAP=100
